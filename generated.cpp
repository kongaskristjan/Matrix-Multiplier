
#include <matrix.hpp>


void Matrix::local_mul8x8(Matrix & ret, const Matrix & ma, const Matrix & mb,
                          size_t x0, size_t y0, size_t n0) const
{
  size_t da = ma.rx, db = mb.rx, dr = ret.rx;
  __m256 r0, r1, r2, r3, r4, r5, r6, r7;
  __m256 a0, a1, a2, a3, b0, bs, bak0, bak1;
  
  float * pr;
  const float * pa, * pb;
  pa = ma.ptr(y0 + 0, n0 + 0);
  pb = mb.ptr(x0 + 0, n0 + 0);
  pr = ret.ptr(x0 + 0, y0 + 0);
  r0 = _mm256_set1_ps(0.0);
  r1 = _mm256_set1_ps(0.0);
  r2 = _mm256_set1_ps(0.0);
  r3 = _mm256_set1_ps(0.0);
  r4 = _mm256_set1_ps(0.0);
  r5 = _mm256_set1_ps(0.0);
  r6 = _mm256_set1_ps(0.0);
  r7 = _mm256_set1_ps(0.0);
  
  a0 = _mm256_load_ps(pa + 0 * da);
  b0 = _mm256_load_ps(pb + 0 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 1 * da);
  b0 = _mm256_load_ps(pb + 1 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 2 * da);
  b0 = _mm256_load_ps(pb + 2 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 3 * da);
  b0 = _mm256_load_ps(pb + 3 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 4 * da);
  b0 = _mm256_load_ps(pb + 4 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 5 * da);
  b0 = _mm256_load_ps(pb + 5 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 6 * da);
  b0 = _mm256_load_ps(pb + 6 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 7 * da);
  b0 = _mm256_load_ps(pb + 7 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pr);
  a1 = _mm256_load_ps(pr + dr);
  a2 = _mm256_load_ps(pr + 2 * dr);
  a3 = _mm256_load_ps(pr + 3 * dr);
  b0 = _mm256_load_ps(pr + 4 * dr);
  bs = _mm256_load_ps(pr + 5 * dr);
  bak0 = _mm256_load_ps(pr + 6 * dr);
  bak1 = _mm256_load_ps(pr + 7 * dr);
  r0 = _mm256_add_ps(r0, a0);
  r1 = _mm256_add_ps(r1, a1);
  r2 = _mm256_add_ps(r2, a2);
  r3 = _mm256_add_ps(r3, a3);
  r4 = _mm256_add_ps(r4, b0);
  r5 = _mm256_add_ps(r5, bs);
  r6 = _mm256_add_ps(r6, bak0);
  r7 = _mm256_add_ps(r7, bak1);
  _mm256_store_ps(pr, r0);
  _mm256_store_ps(pr + dr, r1);
  _mm256_store_ps(pr + 2 * dr, r2);
  _mm256_store_ps(pr + 3 * dr, r3);
  _mm256_store_ps(pr + 4 * dr, r4);
  _mm256_store_ps(pr + 5 * dr, r5);
  _mm256_store_ps(pr + 6 * dr, r6);
  _mm256_store_ps(pr + 7 * dr, r7);
}


void Matrix::local_mul16x16(Matrix & ret, const Matrix & ma, const Matrix & mb,
                            size_t x0, size_t y0, size_t n0) const
{
  size_t da = ma.rx, db = mb.rx, dr = ret.rx;
  __m256 r0, r1, r2, r3, r4, r5, r6, r7;
  __m256 a0, a1, a2, a3, b0, bs, bak0, bak1;
  
  float * pr;
  const float * pa, * pb;
  pa = ma.ptr(y0 + 0, n0 + 0);
  pb = mb.ptr(x0 + 0, n0 + 0);
  pr = ret.ptr(x0 + 0, y0 + 0);
  r0 = _mm256_set1_ps(0.0);
  r1 = _mm256_set1_ps(0.0);
  r2 = _mm256_set1_ps(0.0);
  r3 = _mm256_set1_ps(0.0);
  r4 = _mm256_set1_ps(0.0);
  r5 = _mm256_set1_ps(0.0);
  r6 = _mm256_set1_ps(0.0);
  r7 = _mm256_set1_ps(0.0);
  
  a0 = _mm256_load_ps(pa + 0 * da);
  b0 = _mm256_load_ps(pb + 0 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 1 * da);
  b0 = _mm256_load_ps(pb + 1 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 2 * da);
  b0 = _mm256_load_ps(pb + 2 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 3 * da);
  b0 = _mm256_load_ps(pb + 3 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 4 * da);
  b0 = _mm256_load_ps(pb + 4 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 5 * da);
  b0 = _mm256_load_ps(pb + 5 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 6 * da);
  b0 = _mm256_load_ps(pb + 6 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 7 * da);
  b0 = _mm256_load_ps(pb + 7 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  pa = ma.ptr(y0 + 0, n0 + 8);
  pb = mb.ptr(x0 + 0, n0 + 8);
  pr = ret.ptr(x0 + 0, y0 + 0);
  
  a0 = _mm256_load_ps(pa + 0 * da);
  b0 = _mm256_load_ps(pb + 0 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 1 * da);
  b0 = _mm256_load_ps(pb + 1 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 2 * da);
  b0 = _mm256_load_ps(pb + 2 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 3 * da);
  b0 = _mm256_load_ps(pb + 3 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 4 * da);
  b0 = _mm256_load_ps(pb + 4 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 5 * da);
  b0 = _mm256_load_ps(pb + 5 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 6 * da);
  b0 = _mm256_load_ps(pb + 6 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 7 * da);
  b0 = _mm256_load_ps(pb + 7 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pr);
  a1 = _mm256_load_ps(pr + dr);
  a2 = _mm256_load_ps(pr + 2 * dr);
  a3 = _mm256_load_ps(pr + 3 * dr);
  b0 = _mm256_load_ps(pr + 4 * dr);
  bs = _mm256_load_ps(pr + 5 * dr);
  bak0 = _mm256_load_ps(pr + 6 * dr);
  bak1 = _mm256_load_ps(pr + 7 * dr);
  r0 = _mm256_add_ps(r0, a0);
  r1 = _mm256_add_ps(r1, a1);
  r2 = _mm256_add_ps(r2, a2);
  r3 = _mm256_add_ps(r3, a3);
  r4 = _mm256_add_ps(r4, b0);
  r5 = _mm256_add_ps(r5, bs);
  r6 = _mm256_add_ps(r6, bak0);
  r7 = _mm256_add_ps(r7, bak1);
  _mm256_store_ps(pr, r0);
  _mm256_store_ps(pr + dr, r1);
  _mm256_store_ps(pr + 2 * dr, r2);
  _mm256_store_ps(pr + 3 * dr, r3);
  _mm256_store_ps(pr + 4 * dr, r4);
  _mm256_store_ps(pr + 5 * dr, r5);
  _mm256_store_ps(pr + 6 * dr, r6);
  _mm256_store_ps(pr + 7 * dr, r7);
  pa = ma.ptr(y0 + 8, n0 + 0);
  pb = mb.ptr(x0 + 0, n0 + 0);
  pr = ret.ptr(x0 + 0, y0 + 8);
  r0 = _mm256_set1_ps(0.0);
  r1 = _mm256_set1_ps(0.0);
  r2 = _mm256_set1_ps(0.0);
  r3 = _mm256_set1_ps(0.0);
  r4 = _mm256_set1_ps(0.0);
  r5 = _mm256_set1_ps(0.0);
  r6 = _mm256_set1_ps(0.0);
  r7 = _mm256_set1_ps(0.0);
  
  a0 = _mm256_load_ps(pa + 0 * da);
  b0 = _mm256_load_ps(pb + 0 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 1 * da);
  b0 = _mm256_load_ps(pb + 1 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 2 * da);
  b0 = _mm256_load_ps(pb + 2 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 3 * da);
  b0 = _mm256_load_ps(pb + 3 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 4 * da);
  b0 = _mm256_load_ps(pb + 4 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 5 * da);
  b0 = _mm256_load_ps(pb + 5 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 6 * da);
  b0 = _mm256_load_ps(pb + 6 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 7 * da);
  b0 = _mm256_load_ps(pb + 7 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  pa = ma.ptr(y0 + 8, n0 + 8);
  pb = mb.ptr(x0 + 0, n0 + 8);
  pr = ret.ptr(x0 + 0, y0 + 8);
  
  a0 = _mm256_load_ps(pa + 0 * da);
  b0 = _mm256_load_ps(pb + 0 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 1 * da);
  b0 = _mm256_load_ps(pb + 1 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 2 * da);
  b0 = _mm256_load_ps(pb + 2 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 3 * da);
  b0 = _mm256_load_ps(pb + 3 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 4 * da);
  b0 = _mm256_load_ps(pb + 4 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 5 * da);
  b0 = _mm256_load_ps(pb + 5 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 6 * da);
  b0 = _mm256_load_ps(pb + 6 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 7 * da);
  b0 = _mm256_load_ps(pb + 7 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pr);
  a1 = _mm256_load_ps(pr + dr);
  a2 = _mm256_load_ps(pr + 2 * dr);
  a3 = _mm256_load_ps(pr + 3 * dr);
  b0 = _mm256_load_ps(pr + 4 * dr);
  bs = _mm256_load_ps(pr + 5 * dr);
  bak0 = _mm256_load_ps(pr + 6 * dr);
  bak1 = _mm256_load_ps(pr + 7 * dr);
  r0 = _mm256_add_ps(r0, a0);
  r1 = _mm256_add_ps(r1, a1);
  r2 = _mm256_add_ps(r2, a2);
  r3 = _mm256_add_ps(r3, a3);
  r4 = _mm256_add_ps(r4, b0);
  r5 = _mm256_add_ps(r5, bs);
  r6 = _mm256_add_ps(r6, bak0);
  r7 = _mm256_add_ps(r7, bak1);
  _mm256_store_ps(pr, r0);
  _mm256_store_ps(pr + dr, r1);
  _mm256_store_ps(pr + 2 * dr, r2);
  _mm256_store_ps(pr + 3 * dr, r3);
  _mm256_store_ps(pr + 4 * dr, r4);
  _mm256_store_ps(pr + 5 * dr, r5);
  _mm256_store_ps(pr + 6 * dr, r6);
  _mm256_store_ps(pr + 7 * dr, r7);
  pa = ma.ptr(y0 + 0, n0 + 0);
  pb = mb.ptr(x0 + 8, n0 + 0);
  pr = ret.ptr(x0 + 8, y0 + 0);
  r0 = _mm256_set1_ps(0.0);
  r1 = _mm256_set1_ps(0.0);
  r2 = _mm256_set1_ps(0.0);
  r3 = _mm256_set1_ps(0.0);
  r4 = _mm256_set1_ps(0.0);
  r5 = _mm256_set1_ps(0.0);
  r6 = _mm256_set1_ps(0.0);
  r7 = _mm256_set1_ps(0.0);
  
  a0 = _mm256_load_ps(pa + 0 * da);
  b0 = _mm256_load_ps(pb + 0 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 1 * da);
  b0 = _mm256_load_ps(pb + 1 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 2 * da);
  b0 = _mm256_load_ps(pb + 2 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 3 * da);
  b0 = _mm256_load_ps(pb + 3 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 4 * da);
  b0 = _mm256_load_ps(pb + 4 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 5 * da);
  b0 = _mm256_load_ps(pb + 5 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 6 * da);
  b0 = _mm256_load_ps(pb + 6 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 7 * da);
  b0 = _mm256_load_ps(pb + 7 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  pa = ma.ptr(y0 + 0, n0 + 8);
  pb = mb.ptr(x0 + 8, n0 + 8);
  pr = ret.ptr(x0 + 8, y0 + 0);
  
  a0 = _mm256_load_ps(pa + 0 * da);
  b0 = _mm256_load_ps(pb + 0 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 1 * da);
  b0 = _mm256_load_ps(pb + 1 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 2 * da);
  b0 = _mm256_load_ps(pb + 2 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 3 * da);
  b0 = _mm256_load_ps(pb + 3 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 4 * da);
  b0 = _mm256_load_ps(pb + 4 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 5 * da);
  b0 = _mm256_load_ps(pb + 5 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 6 * da);
  b0 = _mm256_load_ps(pb + 6 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 7 * da);
  b0 = _mm256_load_ps(pb + 7 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pr);
  a1 = _mm256_load_ps(pr + dr);
  a2 = _mm256_load_ps(pr + 2 * dr);
  a3 = _mm256_load_ps(pr + 3 * dr);
  b0 = _mm256_load_ps(pr + 4 * dr);
  bs = _mm256_load_ps(pr + 5 * dr);
  bak0 = _mm256_load_ps(pr + 6 * dr);
  bak1 = _mm256_load_ps(pr + 7 * dr);
  r0 = _mm256_add_ps(r0, a0);
  r1 = _mm256_add_ps(r1, a1);
  r2 = _mm256_add_ps(r2, a2);
  r3 = _mm256_add_ps(r3, a3);
  r4 = _mm256_add_ps(r4, b0);
  r5 = _mm256_add_ps(r5, bs);
  r6 = _mm256_add_ps(r6, bak0);
  r7 = _mm256_add_ps(r7, bak1);
  _mm256_store_ps(pr, r0);
  _mm256_store_ps(pr + dr, r1);
  _mm256_store_ps(pr + 2 * dr, r2);
  _mm256_store_ps(pr + 3 * dr, r3);
  _mm256_store_ps(pr + 4 * dr, r4);
  _mm256_store_ps(pr + 5 * dr, r5);
  _mm256_store_ps(pr + 6 * dr, r6);
  _mm256_store_ps(pr + 7 * dr, r7);
  pa = ma.ptr(y0 + 8, n0 + 0);
  pb = mb.ptr(x0 + 8, n0 + 0);
  pr = ret.ptr(x0 + 8, y0 + 8);
  r0 = _mm256_set1_ps(0.0);
  r1 = _mm256_set1_ps(0.0);
  r2 = _mm256_set1_ps(0.0);
  r3 = _mm256_set1_ps(0.0);
  r4 = _mm256_set1_ps(0.0);
  r5 = _mm256_set1_ps(0.0);
  r6 = _mm256_set1_ps(0.0);
  r7 = _mm256_set1_ps(0.0);
  
  a0 = _mm256_load_ps(pa + 0 * da);
  b0 = _mm256_load_ps(pb + 0 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 1 * da);
  b0 = _mm256_load_ps(pb + 1 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 2 * da);
  b0 = _mm256_load_ps(pb + 2 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 3 * da);
  b0 = _mm256_load_ps(pb + 3 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 4 * da);
  b0 = _mm256_load_ps(pb + 4 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 5 * da);
  b0 = _mm256_load_ps(pb + 5 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 6 * da);
  b0 = _mm256_load_ps(pb + 6 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 7 * da);
  b0 = _mm256_load_ps(pb + 7 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  pa = ma.ptr(y0 + 8, n0 + 8);
  pb = mb.ptr(x0 + 8, n0 + 8);
  pr = ret.ptr(x0 + 8, y0 + 8);
  
  a0 = _mm256_load_ps(pa + 0 * da);
  b0 = _mm256_load_ps(pb + 0 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 1 * da);
  b0 = _mm256_load_ps(pb + 1 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 2 * da);
  b0 = _mm256_load_ps(pb + 2 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 3 * da);
  b0 = _mm256_load_ps(pb + 3 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 4 * da);
  b0 = _mm256_load_ps(pb + 4 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 5 * da);
  b0 = _mm256_load_ps(pb + 5 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 6 * da);
  b0 = _mm256_load_ps(pb + 6 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pa + 7 * da);
  b0 = _mm256_load_ps(pb + 7 * db);
  bs = _mm256_permute2f128_ps(b0, b0, 1);
  a1 = _mm256_permute_ps(a0, 0x39);
  a2 = _mm256_permute_ps(a0, 0x4e);
  a3 = _mm256_permute_ps(a0, 0x93);
  r0 = _mm256_fmadd_ps(a0, b0, r0);
  r1 = _mm256_fmadd_ps(a1, b0, r1);
  r2 = _mm256_fmadd_ps(a2, b0, r2);
  r3 = _mm256_fmadd_ps(a3, b0, r3);
  r4 = _mm256_fmadd_ps(a0, bs, r4);
  r5 = _mm256_fmadd_ps(a1, bs, r5);
  r6 = _mm256_fmadd_ps(a2, bs, r6);
  r7 = _mm256_fmadd_ps(a3, bs, r7);
  
  a0 = _mm256_load_ps(pr);
  a1 = _mm256_load_ps(pr + dr);
  a2 = _mm256_load_ps(pr + 2 * dr);
  a3 = _mm256_load_ps(pr + 3 * dr);
  b0 = _mm256_load_ps(pr + 4 * dr);
  bs = _mm256_load_ps(pr + 5 * dr);
  bak0 = _mm256_load_ps(pr + 6 * dr);
  bak1 = _mm256_load_ps(pr + 7 * dr);
  r0 = _mm256_add_ps(r0, a0);
  r1 = _mm256_add_ps(r1, a1);
  r2 = _mm256_add_ps(r2, a2);
  r3 = _mm256_add_ps(r3, a3);
  r4 = _mm256_add_ps(r4, b0);
  r5 = _mm256_add_ps(r5, bs);
  r6 = _mm256_add_ps(r6, bak0);
  r7 = _mm256_add_ps(r7, bak1);
  _mm256_store_ps(pr, r0);
  _mm256_store_ps(pr + dr, r1);
  _mm256_store_ps(pr + 2 * dr, r2);
  _mm256_store_ps(pr + 3 * dr, r3);
  _mm256_store_ps(pr + 4 * dr, r4);
  _mm256_store_ps(pr + 5 * dr, r5);
  _mm256_store_ps(pr + 6 * dr, r6);
  _mm256_store_ps(pr + 7 * dr, r7);
}
